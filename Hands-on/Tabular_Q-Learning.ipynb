{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import humblerl as hrl\n",
    "from humblerl import Callback, Mind\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Humble RL\n",
    "\n",
    "Straightforward reinforcement learning Python framework. It provides all the boilerplate code needed to implement RL logic (see diagram below) for you.\n",
    "\n",
    "![RL diagram](refs/rl_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "\n",
    "The surface is described using a grid like the following:\n",
    "![Frozen Lake 8x8](refs/frozen_lake.png)\n",
    "```\n",
    "(S: starting point, safe)\n",
    "(F: frozen surface, safe)\n",
    "(H: hole, fall to your doom)\n",
    "(G: goal, where the frisbee is located)\n",
    "```\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mind\n",
    "\n",
    "Think about the Mind as a brain of an Agent. It will tell the Agent what to do. It will observe what environment returns and decide what to do next. Your Mind needs to provide one method:\n",
    "\n",
    "```python\n",
    "def plan(self, state, train_mode, debug_mode):\n",
    "    \"\"\"Do forward pass through agent model (inference/planning) on state.\n",
    "\n",
    "    Args:\n",
    "        state (object): State of environment to inference on.\n",
    "        train_mode (bool): Informs planner whether it's in training or evaluation mode.\n",
    "            E.g. in evaluation it can optimise graph, disable exploration etc.\n",
    "        debug_mode (bool): Informs planner whether it's in debug mode or not.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Actions scores (e.g. unnormalized log probabilities/Q-values/etc.)\n",
    "            possibly raw Artificial Neural Net output i.e. logits.\n",
    "        object (optional): Mind's extra information, passed to 'on_action_planned' callback.\n",
    "            If you will omit it, it will be set to None by default.\n",
    "    \"\"\"\n",
    "\n",
    "    [...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback\n",
    "\n",
    "Callbacks can be pluged into RL loop to listen to events that happend during each iteration:\n",
    "\n",
    "```python\n",
    "class Callback(metaclass=ABCMeta):\n",
    "    \"\"\"Callbacks can be used to listen to events during RL loop execution.\"\"\"\n",
    "\n",
    "    def on_loop_start(self):\n",
    "        \"\"\"Event when loop starts.\n",
    "\n",
    "        Note:\n",
    "            You can assume, that this event occurs before any other event in current loop.\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def on_loop_end(self, is_aborted):\n",
    "        \"\"\"Event when loop finish.\n",
    "\n",
    "        Args:\n",
    "            is_aborted (bool): Flag indication if loop has finished as planed or was terminated.\n",
    "\n",
    "        Note:\n",
    "            You can assume, that this event occurs after specified episodes number or when\n",
    "            loop is terminated manually (e.g. by Ctrl+C).\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def on_episode_start(self, episode, train_mode):\n",
    "        \"\"\"Event when episode starts.\n",
    "\n",
    "        Args:\n",
    "            episode (int): Episode number.\n",
    "            train_mode (bool): Informs whether episode is in training or evaluation mode.\n",
    "\n",
    "        Note:\n",
    "            You can assume, that this event occurs always before any action is taken in episode.\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def on_episode_end(self, episode, train_mode):\n",
    "        \"\"\"Event after environment was reset.\n",
    "\n",
    "        Args:\n",
    "            episode (int): Episode number.\n",
    "            train_mode (bool): Informs whether episode is in training or evaluation mode.\n",
    "\n",
    "        Note:\n",
    "            You can assume, that this event occurs after step to terminal state.\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def on_action_planned(self, step, logits, info):\n",
    "        \"\"\"Event after Mind was evaluated.\n",
    "\n",
    "        Args:\n",
    "            step (int): Step number.\n",
    "            logits (np.array): Actions scores (e.g. unnormalized log probabilities/Q-values/etc.)\n",
    "                raw values returned from 'Mind.plan(...)'.\n",
    "            info (object): Mind's extra information, may be None.\n",
    "\n",
    "        Note:\n",
    "            You can assume, that this event occurs always before step finish.\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def on_step_taken(self, step, transition, info):\n",
    "        \"\"\"Event after action was taken in environment.\n",
    "\n",
    "        Args:\n",
    "            step (int): Step number.\n",
    "            transition (Transition): Describes transition that took place.\n",
    "            info (object): Environment diagnostic information if available otherwise None.\n",
    "\n",
    "        Note:\n",
    "            Transition is returned from `ply` function (look to docstring for more info).\n",
    "            Also, you can assume, that this event occurs always after action was planned.\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        \"\"\"Returns execution metrics.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary with logs names and values.\n",
    "\n",
    "        Note:\n",
    "            Those values are fetched by 'humblerl.loop(...)' at the end of each episode (after\n",
    "            'on_episode_end is' called) and then returned from 'humblerl.loop(...)' as evaluation\n",
    "            history. Those also are logged by 'humblerl.loop(...)' depending on its verbosity.\n",
    "        \"\"\"\n",
    "\n",
    "        return {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Frozen Lake 8x8](refs/q_learn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularQLearning(Mind, Callback):\n",
    "    def __init__(self, nstates, nactions, learning_rate=0.9, decay_steps=300, discount_factor=0.95):\n",
    "        # Store training parameters\n",
    "        self._lr = learning_rate\n",
    "        self._decay = decay_steps\n",
    "        self._gamma = discount_factor\n",
    "        self._episode_count = 1\n",
    "        self._return = 0\n",
    "        self.running_avg = [0]\n",
    "\n",
    "        # Initialize Q-table\n",
    "        # EXERCISE STARTS HERE\n",
    "        self.Q = np.zeros((..., ...), dtype=np.float)\n",
    "        # EXERCISE ENDS HERE\n",
    "\n",
    "    def plan(self, state, train_mode, debug_mode):\n",
    "        # Decaying over time random noise for exploration (shape: number of actions)\n",
    "        random_noise = np.random.randn(self.Q.shape[1]) * (1. / self._episode_count)\n",
    "        # EXERCISE STARTS HERE\n",
    "        return ... + ...\n",
    "        # EXERCISE ENDS HERE\n",
    "\n",
    "    def on_episode_start(self, episode, train_mode):\n",
    "        self._return = 0\n",
    "\n",
    "    def on_step_taken(self, step, transition, info):\n",
    "        # Add reward to return\n",
    "        self._return += transition.reward\n",
    "\n",
    "        # Exponentially decaying learning rate\n",
    "        LR = pow(self._lr, self._episode_count / self._decay)\n",
    "\n",
    "        # Update Q-table\n",
    "        # EXERCISE STARTS HERE\n",
    "        if transition.is_terminal:\n",
    "            target = ...\n",
    "        else:\n",
    "            target = ...\n",
    "        # EXERCISE ENDS HERE\n",
    "        \n",
    "        # EXERCISE STARTS HERE\n",
    "        self.Q[transition.state, transition.action] += ...\n",
    "        # EXERCISE ENDS HERE\n",
    "\n",
    "        # Count episodes\n",
    "        if transition.is_terminal:\n",
    "            self._episode_count += 1\n",
    "\n",
    "    def on_episode_end(self, episode, train_mode):\n",
    "        self.running_avg.append(0.01 * self._return + 0.99 * self.running_avg[-1])\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return {\"avg. return\": self.running_avg[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper params\n",
    "episodes = 865\n",
    "lr = 0.75\n",
    "decay = 400\n",
    "gamma = 0.95\n",
    "\n",
    "# Create environment and q-learning agent\n",
    "env = hrl.create_gym(\"FrozenLake-v0\")\n",
    "mind = TabularQLearning(env.state_space, env.action_space.num,\n",
    "                        learning_rate=lr, decay_steps=decay, discount_factor=gamma)\n",
    "\n",
    "# Seed env and numpy\n",
    "np.random.seed(7)\n",
    "env.env.seed(7)\n",
    "\n",
    "# Run training\n",
    "_ = hrl.loop(env, mind, n_episodes=episodes, callbacks=[mind], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot episodes lengths and returns\n",
    "plt.plot(mind.running_avg, label=\"Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Frozen Lake 8x8](refs/frozen_lake.png)\n",
    "### Actions:\n",
    "* 0 - Left\n",
    "* 1 - Down\n",
    "* 2 - Right\n",
    "* 3 - Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(mind.Q, axis=1).reshape(4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is he going right in the (3,2) position?! Straight into the hole!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
