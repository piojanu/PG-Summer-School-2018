{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 12,12\n",
    "\n",
    "from utils import MDP, visualize\n",
    "\n",
    "# Use seaborn style plots\n",
    "sns.set()\n",
    "\n",
    "# Set random generators seeds\n",
    "np.random.seed(7)\n",
    "\n",
    "# \"Create problem\"\n",
    "mdp = MDP()\n",
    "T = mdp.get_dynamics(path='data.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jack’s Car Rental Problem\n",
    "#####  \n",
    "![frozen lake](refs/car_rent.jpg)\n",
    "#####  \n",
    "\n",
    ">Jack manages **two locations** for a nationwide car rental company. Each day, some number of customers arrive at each location to rent cars. If Jack has a car available, he **rents it out and is credited \\$10** by the national company. If he is **out of cars at that location, then the business is lost**. Cars become available for renting the **day after they are returned**. To help ensure that cars are available where they are needed, Jack can move them between the two locations overnight, at a cost of **\\$2 per car moved**. We assume that the number of **cars requested and returned at each location are Poisson random variables**, meaning that the probability that the number is $n$ is $\\frac{λ^n}{n!}e^{−λ}$, where $λ$ is the expected number. Suppose **$λ$ is 3 and 4 for rental requests** at the first and second locations and **$3$ and $2$ for returns**. To simplify the problem slightly, we assume that there can be **no more than 20 cars at each location** (any additional cars are returned to the nationwide company, and thus **disappear from the problem**) and a maximum of **five cars can be moved** from one location to the other in one night. We take the **discount rate to be $γ = 0.9$** and formulate this as a **continuing finite MDP** [...]\"~ _Reinforcement Learning: An Introduction 2nd_\n",
    "\n",
    "Note that Poisson distribution tells us probability of given number of Poisson processes (independent events) occurring in fixed interval of time, when mean number of Poisson processes occurring in this interval is known.\n",
    "\n",
    "## Checklist questions:\n",
    "* What is STATE in this problem?\n",
    "* What is ACTION in this problem?\n",
    "* What is REWARD in this problem?\n",
    "* What is TRANSITION in this problem?\n",
    "* What is EPISODE in this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy, value and dynamics arrays:\n",
    "\n",
    "* `pi[<num loc. 1>, <num loc. 2>] -> action`\n",
    "\n",
    "* `V[<num loc. 1>, <num loc. 2>] -> expected return / value`\n",
    "\n",
    "* `T[<num loc. 1>, <num loc. 2>, <num cars to move>, <next num loc. 1>, <next num loc. 2>] -> prob., reward`  \n",
    "  We consider cars number in each location AFTER day ends, but BEFORE any car is moved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SPACE = T.shape[:2]\n",
    "print(\"State space shape: \", STATE_SPACE)\n",
    "ACTION_SPACE = T.shape[2]\n",
    "print(\"Action space size: \", ACTION_SPACE)\n",
    "\n",
    "# How big is out problem?\n",
    "print(\"State-action pairs: \", T[:,:,:,0,0,0].size)\n",
    "print(\"Transitions: \", T[:,:,:,:,:,0].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation\n",
    "\n",
    "Value function tells us how good current policy is in terms of state values. Policy evaluation gives us (approximation) of current policy value function.\n",
    "\n",
    "![value interation](refs/policy_eval.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize arbitrary policy and state values\n",
    "pi = np.zeros(STATE_SPACE, dtype=np.int)\n",
    "V = np.zeros(STATE_SPACE, dtype=np.float)\n",
    "\n",
    "def q_value(n1, n2, action, gamma):\n",
    "    q = 0.\n",
    "    # For each next state\n",
    "    for n1_, n2_ in mdp.states_iter():\n",
    "        ## EXERCISE STARTS HERE ##\n",
    "        p, r = T[...]\n",
    "        q += ...\n",
    "        ## EXERCISE ENDS HERE ##\n",
    "    \n",
    "    return q\n",
    "\n",
    "def policy_evaluation(gamma, theta, visualize_each=0):\n",
    "    i = 0\n",
    "    while True:\n",
    "        ## DO NOT TOUCH, state values visualization ##\n",
    "        if visualize_each > 0 and i % visualize_each == 0:\n",
    "            print(\"Policy evaluation: {}\".format(i))\n",
    "            visualize(V=V)\n",
    "            plt.pause(0.05)\n",
    "        \n",
    "        V_old = np.copy(V)\n",
    "        for n1, n2 in mdp.states_iter():\n",
    "            ## EXERCISE STARTS HERE ##\n",
    "            ... = q_value(..., gamma=gamma)\n",
    "            ## EXERCISE ENDS HERE ##\n",
    "\n",
    "        # Early stopping\n",
    "        max_diff = np.max(np.abs(V_old - V))\n",
    "        if max_diff < theta:\n",
    "            print(\"Policy evaluation finished in {} steps\".format(i + 1))\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "policy_evaluation(gamma=0.9, theta=1., visualize_each=5)\n",
    "visualize(V=V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Improvement\n",
    "\n",
    "It utilizes one step lookahead to improve current policy based on state values. This is greedy policy which takes the action that looks best in the short term according to current value function.\n",
    "\n",
    "![policy interation](refs/policy_improv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(gamma):\n",
    "    changed = False\n",
    "    for n1, n2 in mdp.states_iter():\n",
    "        # Get legal actions\n",
    "        actions = list(mdp.actions_in_state(n1, n2))\n",
    "        \n",
    "        # Get state-action value for each legal action\n",
    "        q_values = list(map(lambda action: q_value(n1, n2, action, gamma=gamma), actions))\n",
    "        \n",
    "        ## EXERCISE STARTS HERE ##\n",
    "        best_a = ...[np.argmax(...)]\n",
    "        if ... != best_a:\n",
    "            ... = best_a\n",
    "            changed = True\n",
    "        ## EXERCISE ENDS HERE ##\n",
    "            \n",
    "    return changed\n",
    "\n",
    "policy_improvement(gamma=0.9)\n",
    "visualize(pi=pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "\n",
    "We evaluate current policy to get current state values and then based on those new values improve policy to take better actions in the future. Then we evaluate better policy to get new \"better\" states values and then... process iterates until policy doesn't change any more.\n",
    "\n",
    "![policy interation](refs/policy_iter.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize arbitrary policy and state values\n",
    "pi = np.zeros(STATE_SPACE, dtype=np.int)\n",
    "V = np.zeros(STATE_SPACE, dtype=np.float)\n",
    "\n",
    "def policy_iteration(gamma=0.9, theta=1.):\n",
    "    i = 0\n",
    "    while True:\n",
    "        ## DO NOT TOUCH, policy and state value visualization ##\n",
    "        print(\"Policy iteration step: {}\".format(i))\n",
    "        visualize(pi=pi, V=V)\n",
    "        plt.pause(0.05)\n",
    "        \n",
    "        ## EXERCISE STARTS HERE ##\n",
    "        ...(gamma=gamma, theta=theta)\n",
    "        changed = ...(gamma=gamma)\n",
    "        ## EXERCISE ENDS HERE ##\n",
    "        \n",
    "        if not changed:\n",
    "            print(\"Policy iteration finished in {} iterations!\".format(i + 1))\n",
    "            visualize(pi=pi, V=V)\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "policy_iteration()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
